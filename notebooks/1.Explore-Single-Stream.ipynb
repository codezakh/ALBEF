{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d516139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "896ce37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"/home/zaid/Source/ALBEF/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7642d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vit import VisionTransformer\n",
    "from transformers import BertModelForMaskedLM, AutoTokenizer, BertConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18b43ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') \n",
    "text = \"this is a random image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de93ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = BertModel(BertConfig())\n",
    "vm = VisionTransformer()\n",
    "tokens = torch.Tensor(tokenizer.encode(text)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b5d5b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 2023, 2003, 1037, 6721, 3746,  102])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224553a9",
   "metadata": {},
   "source": [
    "# How does ALBEF handle tokenization?\n",
    "The captions in the input JSON fed into ALBEF are raw text. They are loaded into the [`pretrain_dataset`](https://sourcegraph.com/github.com/salesforce/ALBEF/-/blob/dataset/caption_dataset.py?L97) class, where the `pre_caption` function does some basic preprocessing. \n",
    "It's then wrapped by a `create_dataset` function that doesn't alter the text data.\n",
    "So, in summary, the dataset that comes out of `create_dataset` has `str` typed text data, not integer input ids.\n",
    "The dataset then gets passed into a `create_loader` function, which also does not modify the text data. \n",
    "The tokenization happens in the [`train`](https://sourcegraph.com/github.com/salesforce/ALBEF@9e9a5e952f72374c15cea02d3c34013554c86513/-/blob/Pretrain.py?L59) function.\n",
    "\n",
    "```python\n",
    "text_input = tokenizer(text, padding='longest', truncation=True, max_length=25, return_tensors=\"pt\").to(device)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65819f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"this\",\n",
    "    \"this is\",\n",
    "    \"this is a\",\n",
    "    \"this is a random\",\n",
    "    \"this is a random image\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fd147c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(text, padding='longest', truncation=True, max_length=25, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6ecda2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023,  102,    0,    0,    0,    0],\n",
       "        [ 101, 2023, 2003,  102,    0,    0,    0],\n",
       "        [ 101, 2023, 2003, 1037,  102,    0,    0],\n",
       "        [ 101, 2023, 2003, 1037, 6721,  102,    0],\n",
       "        [ 101, 2023, 2003, 1037, 6721, 3746,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff01b2",
   "metadata": {},
   "source": [
    "# How does sentence-pair tokenization look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10fddda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer.batch_encode_plus(list(zip(text, text)), padding='longest', truncation=True, max_length=50, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5eefd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023,  102, 2023,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0],\n",
       "        [ 101, 2023, 2003,  102, 2023, 2003,  102,    0,    0,    0,    0,    0,\n",
       "            0],\n",
       "        [ 101, 2023, 2003, 1037,  102, 2023, 2003, 1037,  102,    0,    0,    0,\n",
       "            0],\n",
       "        [ 101, 2023, 2003, 1037, 6721,  102, 2023, 2003, 1037, 6721,  102,    0,\n",
       "            0],\n",
       "        [ 101, 2023, 2003, 1037, 6721, 3746,  102, 2023, 2003, 1037, 6721, 3746,\n",
       "          102]]), 'token_type_ids': tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa1dee",
   "metadata": {},
   "source": [
    "The input ALBEF gets is a dictionary containing `input_ids` and the `attention_mask`. The attention mask covers all the non-pad tokens. In sentence-pair mode, there is no padding in between the sentences. The sentences are just separated by the `[SEP]` symbol. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4884d072",
   "metadata": {},
   "source": [
    "# Constructing a sentence pair from ViT / word embeddings\n",
    "The ViT output sequence is always the same length, and if stacked together, has no ragged edges. The sentences cannot be stacked together without padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecbc2080",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.rand(3, 224, 224).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11b0844a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_out = vm(image)\n",
    "vit_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4af072",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"this\",\n",
    "    \"this is\",\n",
    "    \"this is a\",\n",
    "    \"this is a random\",\n",
    "    \"this is a random image\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb2161c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 197, 768])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a batch the same size as the amount of text.\n",
    "img_batch = torch.vstack([vit_out] * len(text))\n",
    "img_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dfbf3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text.\n",
    "text_batch = tokenizer(text, padding='longest', truncation=True, max_length=25, return_tensors=\"pt\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "be7679c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[2023,    0,    0,    0,    0],\n",
       "        [2023, 2003,    0,    0,    0],\n",
       "        [2023, 2003, 1037,    0,    0],\n",
       "        [2023, 2003, 1037, 6721,    0],\n",
       "        [2023, 2003, 1037, 6721, 3746]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d87bfe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = torch.ones(batch_size, 1, 1) * tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9aa25ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = torch.ones(batch_size, 1, 1) * tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf094f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos = torch.ones(batch_size, 1, 1) * tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d94c0",
   "metadata": {},
   "source": [
    "The `input_embeds` keyword, which we will be passing data into, is filled from the `word_embeddings` layer if not provided (https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert/modeling_bert.py#L214)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "41d1d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_input_embeds = lm.embeddings.word_embeddings(text_batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6aeabf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_embeds = lm.embeddings.word_embeddings(prefix.long()).squeeze(1)\n",
    "sep_embeds = lm.embeddings.word_embeddings(separator.long()).squeeze(1)\n",
    "eos_embeds = lm.embeddings.word_embeddings(eos.long()).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e0e3844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = torch.cat([prefix_embeds, img_batch, sep_embeds, lang_input_embeds, eos_embeds], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f032ca88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-4.7683e-01, -6.3524e-01, -4.1642e-01,  ..., -9.7788e-01,\n",
       "           1.8886e-01,  1.7899e+00],\n",
       "         [-4.3331e-01,  3.0411e-01,  6.5908e-01,  ...,  5.7775e-01,\n",
       "           7.4744e-01,  2.0552e+00],\n",
       "         [-7.9278e-01, -7.9437e-01,  4.4264e-01,  ...,  6.4663e-01,\n",
       "           7.7473e-01,  4.7329e-01],\n",
       "         ...,\n",
       "         [-1.7355e+00, -4.6115e-01,  1.0934e+00,  ...,  4.5150e-01,\n",
       "           9.1559e-01,  1.2327e+00],\n",
       "         [-6.4930e-01, -8.7906e-01, -4.5566e-01,  ..., -2.5346e-04,\n",
       "          -2.1459e-01,  2.0685e+00],\n",
       "         [-6.2172e-01, -5.5700e-01,  7.0330e-01,  ..., -4.7587e-01,\n",
       "           5.4234e-01,  1.2120e+00]],\n",
       "\n",
       "        [[-4.2127e-01, -3.4930e-01,  4.4559e-01,  ..., -3.3558e-01,\n",
       "          -3.7299e-01,  1.3806e+00],\n",
       "         [-2.9046e-01,  1.5188e-01,  3.7710e-01,  ...,  5.8053e-01,\n",
       "           1.9641e+00,  6.7893e-01],\n",
       "         [ 1.1188e-01, -7.8049e-01,  2.9377e-01,  ...,  4.1177e-01,\n",
       "          -3.2631e-01,  1.0834e+00],\n",
       "         ...,\n",
       "         [-1.0684e+00,  2.0179e-01,  2.0094e-01,  ...,  5.7924e-01,\n",
       "           1.4194e+00,  2.3621e+00],\n",
       "         [-5.7513e-01, -2.1225e+00,  3.6720e-01,  ...,  2.8497e-02,\n",
       "           4.4669e-01,  1.4849e+00],\n",
       "         [-1.0182e+00,  7.6042e-01,  3.4884e-01,  ..., -3.7049e-01,\n",
       "           7.9410e-01,  2.2709e+00]],\n",
       "\n",
       "        [[ 1.3649e-01, -1.0654e+00, -2.5547e-01,  ..., -1.3446e+00,\n",
       "           9.4433e-01,  2.1271e+00],\n",
       "         [-5.4627e-01, -4.5734e-01,  1.0554e+00,  ...,  2.2435e-01,\n",
       "           1.3109e+00,  1.6850e+00],\n",
       "         [-1.0831e+00,  4.1132e-03, -4.0275e-01,  ..., -2.3874e-01,\n",
       "           2.2683e-01,  2.1635e+00],\n",
       "         ...,\n",
       "         [-1.3186e+00, -3.0518e-01,  5.5907e-01,  ...,  6.2212e-01,\n",
       "           6.6271e-01,  2.3327e+00],\n",
       "         [-4.0184e-01, -1.8243e+00,  5.2188e-01,  ..., -4.0912e-01,\n",
       "          -1.4246e-01,  1.0484e+00],\n",
       "         [-6.1314e-01,  1.0378e-01, -5.2970e-01,  ..., -1.3562e-01,\n",
       "           5.2606e-02,  1.6002e+00]],\n",
       "\n",
       "        [[-9.7903e-02, -1.6200e+00, -1.4984e-02,  ..., -5.9587e-01,\n",
       "           7.4790e-01,  1.7775e+00],\n",
       "         [-6.7340e-01, -1.3198e+00,  1.1285e+00,  ...,  2.9818e-01,\n",
       "           9.7698e-01,  1.0728e+00],\n",
       "         [-3.6570e-01, -4.0383e-01, -7.5702e-01,  ...,  2.7584e-01,\n",
       "           1.3352e+00,  6.7749e-01],\n",
       "         ...,\n",
       "         [-7.3827e-01,  4.8359e-02,  3.8007e-01,  ..., -6.8189e-01,\n",
       "           6.5084e-01,  2.0174e+00],\n",
       "         [ 2.9729e-01, -1.0163e+00,  4.3926e-01,  ..., -2.2940e-01,\n",
       "           3.8724e-01,  1.8491e+00],\n",
       "         [ 2.7256e-01, -8.8633e-01, -4.0131e-01,  ...,  2.1769e-01,\n",
       "           1.4918e-01,  1.3985e+00]],\n",
       "\n",
       "        [[-2.0882e-01, -1.5985e+00, -4.7434e-01,  ..., -1.3950e+00,\n",
       "          -1.4254e-01,  2.9795e+00],\n",
       "         [-1.1581e+00, -1.3735e-01,  4.6483e-01,  ..., -1.7469e-01,\n",
       "           1.4149e+00,  1.4004e+00],\n",
       "         [-6.9806e-01, -2.0082e-01, -2.7335e-02,  ...,  6.3186e-01,\n",
       "           1.4458e+00,  7.7687e-01],\n",
       "         ...,\n",
       "         [-1.3826e+00, -7.9548e-01,  5.0034e-01,  ...,  7.4576e-01,\n",
       "           6.0867e-01,  1.7333e+00],\n",
       "         [ 1.2215e-02, -5.5145e-01, -1.0128e-01,  ...,  1.2034e-01,\n",
       "           7.5150e-01,  2.3480e+00],\n",
       "         [-7.8016e-01, -1.6389e-01,  6.7742e-01,  ...,  4.7751e-02,\n",
       "           9.6304e-01,  1.5887e+00]]], grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.0681, -0.2049,  0.2373,  ...,  0.9306,  0.7346, -0.0761],\n",
       "        [-0.3692, -0.0412, -0.0873,  ...,  0.9287,  0.5606, -0.0348],\n",
       "        [ 0.2432, -0.1928,  0.2297,  ...,  0.9530,  0.7575, -0.1766],\n",
       "        [ 0.3675, -0.1181,  0.1052,  ...,  0.9327,  0.7483,  0.1148],\n",
       "        [-0.1322, -0.1980, -0.1983,  ...,  0.9015,  0.7896, -0.0087]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(inputs_embeds=model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190d791",
   "metadata": {},
   "source": [
    "## Computing the attention and token_type_id masks\n",
    "Let's ignore this for now and see what happens. It could be unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea13ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
