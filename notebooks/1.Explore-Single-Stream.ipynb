{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d516139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "896ce37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"/home/zaid/Source/ALBEF/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f7642d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vit import VisionTransformer\n",
    "from transformers import BertForMaskedLM, AutoTokenizer, BertConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18b43ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') \n",
    "text = \"this is a random image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "de93ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = BertForMaskedLM(BertConfig())\n",
    "vm = VisionTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224553a9",
   "metadata": {},
   "source": [
    "# How does ALBEF handle tokenization?\n",
    "The captions in the input JSON fed into ALBEF are raw text. They are loaded into the [`pretrain_dataset`](https://sourcegraph.com/github.com/salesforce/ALBEF/-/blob/dataset/caption_dataset.py?L97) class, where the `pre_caption` function does some basic preprocessing. \n",
    "It's then wrapped by a `create_dataset` function that doesn't alter the text data.\n",
    "So, in summary, the dataset that comes out of `create_dataset` has `str` typed text data, not integer input ids.\n",
    "The dataset then gets passed into a `create_loader` function, which also does not modify the text data. \n",
    "The tokenization happens in the [`train`](https://sourcegraph.com/github.com/salesforce/ALBEF@9e9a5e952f72374c15cea02d3c34013554c86513/-/blob/Pretrain.py?L59) function.\n",
    "\n",
    "```python\n",
    "text_input = tokenizer(text, padding='longest', truncation=True, max_length=25, return_tensors=\"pt\").to(device)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65819f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"this\",\n",
    "    \"this is\",\n",
    "    \"this is a\",\n",
    "    \"this is a random\",\n",
    "    \"this is a random image\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c3e37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(text, padding='longest', truncation=True, max_length=25, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "128245d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023,  102,    0,    0,    0,    0],\n",
       "        [ 101, 2023, 2003,  102,    0,    0,    0],\n",
       "        [ 101, 2023, 2003, 1037,  102,    0,    0],\n",
       "        [ 101, 2023, 2003, 1037, 6721,  102,    0],\n",
       "        [ 101, 2023, 2003, 1037, 6721, 3746,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196794e2",
   "metadata": {},
   "source": [
    "# How does sentence-pair tokenization look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af1c68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer.batch_encode_plus(list(zip(text, text)), padding='longest', truncation=True, max_length=50, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b6e0d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023,  102, 2023,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0],\n",
       "        [ 101, 2023, 2003,  102, 2023, 2003,  102,    0,    0,    0,    0,    0,\n",
       "            0],\n",
       "        [ 101, 2023, 2003, 1037,  102, 2023, 2003, 1037,  102,    0,    0,    0,\n",
       "            0],\n",
       "        [ 101, 2023, 2003, 1037, 6721,  102, 2023, 2003, 1037, 6721,  102,    0,\n",
       "            0],\n",
       "        [ 101, 2023, 2003, 1037, 6721, 3746,  102, 2023, 2003, 1037, 6721, 3746,\n",
       "          102]]), 'token_type_ids': tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58aafa7",
   "metadata": {},
   "source": [
    "The input ALBEF gets is a dictionary containing `input_ids` and the `attention_mask`. The attention mask covers all the non-pad tokens. In sentence-pair mode, there is no padding in between the sentences. The sentences are just separated by the `[SEP]` symbol. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b74eabc",
   "metadata": {},
   "source": [
    "# Constructing a sentence pair from ViT / word embeddings\n",
    "The ViT output sequence is always the same length, and if stacked together, has no ragged edges. The sentences cannot be stacked together without padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91556a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.rand(3, 224, 224).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e451ec24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_out = vm(image)\n",
    "vit_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be784d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"this\",\n",
    "    \"this is\",\n",
    "    \"this is a\",\n",
    "    \"this is a random\",\n",
    "    \"this is a random image\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a40a2cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 197, 768])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a batch the same size as the amount of text.\n",
    "img_batch = torch.vstack([vit_out] * len(text))\n",
    "img_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "02004bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text.\n",
    "text_batch = tokenizer(text, padding='longest', truncation=True, max_length=25, return_tensors=\"pt\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "39b32504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[2023,    0,    0,    0,    0],\n",
       "        [2023, 2003,    0,    0,    0],\n",
       "        [2023, 2003, 1037,    0,    0],\n",
       "        [2023, 2003, 1037, 6721,    0],\n",
       "        [2023, 2003, 1037, 6721, 3746]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7fe2f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = torch.ones(batch_size, 1, 1) * tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "218af9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = torch.ones(batch_size, 1, 1) * tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos = torch.ones(batch_size, 1, 1) * tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c1b68",
   "metadata": {},
   "source": [
    "The `input_embeds` keyword, which we will be passing data into, is filled from the `word_embeddings` layer if not provided (https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert/modeling_bert.py#L214)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "722088e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_input_embeds = lm.embeddings.word_embeddings(text_batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "85d5deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_embeds = lm.embeddings.word_embeddings(prefix.long()).squeeze(1)\n",
    "sep_embeds = lm.embeddings.word_embeddings(separator.long()).squeeze(1)\n",
    "eos_embeds = lm.embeddings.word_embeddings(eos.long()).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c54c7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = torch.cat([prefix_embeds, img_batch, sep_embeds, lang_input_embeds, eos_embeds], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5a5c8e05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 1.6899e+00, -5.4230e-01,  3.5317e-01,  ...,  6.5037e-01,\n",
       "          -1.2622e+00, -3.3358e-01],\n",
       "         [-3.1596e-02,  4.0642e-01,  5.5573e-01,  ..., -1.4848e-01,\n",
       "          -8.9354e-01, -1.0798e-02],\n",
       "         [ 1.3592e-01,  1.2805e-01, -6.3903e-01,  ..., -7.1887e-01,\n",
       "          -1.1928e+00, -1.4440e+00],\n",
       "         ...,\n",
       "         [ 3.7314e-01,  6.8338e-01, -2.9855e-01,  ...,  5.7508e-01,\n",
       "          -3.7531e-01,  6.8136e-01],\n",
       "         [ 3.4729e-01,  9.2582e-01,  2.2197e-01,  ..., -7.3205e-01,\n",
       "          -2.8642e-01, -4.6515e-01],\n",
       "         [-4.6991e-02, -1.1992e+00,  9.2487e-01,  ...,  6.4147e-01,\n",
       "          -3.8263e-01,  5.7462e-01]],\n",
       "\n",
       "        [[ 1.5546e+00, -4.8769e-01,  1.3554e-01,  ..., -8.3493e-01,\n",
       "          -2.3868e-01,  2.5389e-03],\n",
       "         [ 4.6983e-01,  1.2182e-01,  7.3298e-02,  ...,  4.8344e-01,\n",
       "          -3.0528e-01, -4.7187e-01],\n",
       "         [ 5.5295e-01, -7.9896e-02, -8.7515e-01,  ...,  5.2899e-02,\n",
       "          -5.5770e-01,  3.2232e-01],\n",
       "         ...,\n",
       "         [ 1.6431e+00,  2.2709e-01,  2.1040e-01,  ...,  8.6991e-01,\n",
       "          -7.5119e-01,  1.9016e-02],\n",
       "         [ 3.4280e-01,  4.6866e-01, -1.9872e-02,  ..., -3.6368e-01,\n",
       "          -8.4361e-02, -5.8658e-01],\n",
       "         [ 1.2333e+00, -3.5895e-01,  5.6069e-01,  ...,  3.7301e-01,\n",
       "          -3.6612e-02, -2.2876e-01]],\n",
       "\n",
       "        [[ 5.6219e-01, -8.4844e-01,  6.8106e-01,  ..., -8.4521e-01,\n",
       "          -2.5211e-01, -7.3114e-01],\n",
       "         [-5.8904e-01,  5.8851e-01, -9.9325e-02,  ..., -1.9144e-01,\n",
       "          -1.1081e+00, -6.4735e-04],\n",
       "         [ 5.3054e-01,  2.2395e-01, -1.5531e-01,  ..., -6.0334e-01,\n",
       "          -1.4009e-02, -2.2468e-01],\n",
       "         ...,\n",
       "         [ 9.4683e-01,  1.1484e+00,  5.6347e-01,  ...,  9.9336e-01,\n",
       "          -4.3432e-01, -3.9984e-01],\n",
       "         [ 1.1124e-01,  8.5505e-01,  4.7938e-01,  ..., -3.1988e-01,\n",
       "          -5.6106e-02,  2.2788e-01],\n",
       "         [ 4.3561e-01, -1.7988e-01,  2.8444e-01,  ...,  1.5178e-02,\n",
       "           7.5563e-02, -2.3892e-01]],\n",
       "\n",
       "        [[ 1.7502e+00,  6.7756e-02, -1.2637e-01,  ...,  9.6935e-02,\n",
       "          -8.0888e-01, -3.0211e-01],\n",
       "         [-1.6007e-02,  1.7036e-01, -1.6487e-01,  ...,  2.4399e-01,\n",
       "          -5.2312e-01, -8.8492e-02],\n",
       "         [ 1.2097e+00,  3.1162e-01, -2.5039e-01,  ..., -1.5986e+00,\n",
       "          -2.0473e-01, -2.3543e-01],\n",
       "         ...,\n",
       "         [ 1.6863e+00,  6.8057e-01,  7.0292e-01,  ..., -7.0639e-01,\n",
       "           1.2245e-01, -1.8276e-01],\n",
       "         [ 8.6452e-01,  4.6997e-01,  8.6577e-01,  ..., -1.6278e-01,\n",
       "           1.4430e-01, -1.6982e-01],\n",
       "         [-8.8880e-01, -8.2261e-01,  7.9758e-01,  ..., -4.4641e-01,\n",
       "          -9.1072e-01,  4.1438e-01]],\n",
       "\n",
       "        [[ 1.4284e+00, -5.5846e-01, -5.6983e-01,  ..., -1.4443e-01,\n",
       "          -6.4804e-01,  4.1293e-02],\n",
       "         [ 7.1653e-01,  1.4682e+00,  7.8438e-01,  ...,  1.9158e-01,\n",
       "          -4.8899e-01, -6.0516e-02],\n",
       "         [ 1.3335e+00, -3.7012e-01, -1.3063e-01,  ..., -4.9685e-01,\n",
       "          -1.0538e-01,  8.3525e-01],\n",
       "         ...,\n",
       "         [ 2.6296e-01,  5.0372e-01, -8.8551e-01,  ...,  2.5488e-01,\n",
       "          -5.5479e-01, -1.4138e+00],\n",
       "         [ 8.2413e-01,  1.1186e+00,  9.9886e-01,  ...,  6.8066e-01,\n",
       "           4.4462e-01, -9.4985e-01],\n",
       "         [ 1.8474e-01, -1.0818e+00,  3.2711e-01,  ..., -1.0041e-01,\n",
       "          -2.2111e-01,  4.4140e-01]]], grad_fn=<NativeLayerNormBackward>), pooler_output=None, hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.bert(inputs_embeds=model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e47a2",
   "metadata": {},
   "source": [
    "## Computing the  token_type_id mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "27738878",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pair = tokenizer.encode_plus(\n",
    "    text=\"this is some text\",\n",
    "    text_pair=\"a second sentence, longer than the first\",\n",
    "    padding='longest', \n",
    "    truncation=True, \n",
    "    max_length=25, \n",
    "    return_tensors=\"pt\", \n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "11d0a4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2023, 2003, 2070, 3793,  102, 1037, 2117, 6251, 1010, 2936, 2084,\n",
      "         1996, 2034,  102]])\n",
      "torch.Size([1, 15])\n"
     ]
    }
   ],
   "source": [
    "print(sentence_pair.input_ids)\n",
    "print(sentence_pair.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7c98a8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([1, 15])\n"
     ]
    }
   ],
   "source": [
    "print(sentence_pair.attention_mask)\n",
    "print(sentence_pair.attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "abcff731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([1, 15])\n"
     ]
    }
   ],
   "source": [
    "print(sentence_pair.token_type_ids)\n",
    "print(sentence_pair.token_type_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ef600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
